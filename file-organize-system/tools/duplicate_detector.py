#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡ä»¶é‡å¤æ£€æµ‹å·¥å…·
åŸºäºæ–‡ä»¶å†…å®¹çš„MD5å’ŒSHA256å“ˆå¸Œå€¼æ£€æµ‹é‡å¤æ–‡ä»¶
æ”¯æŒJSONæ ¼å¼è¾“å‡ºæŠ¥å‘Šï¼Œä¾¿äºåç»­å¤„ç†
"""

import os
import hashlib
import json
import argparse
import time
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Set

class DuplicateFileDetector:
    """é‡å¤æ–‡ä»¶æ£€æµ‹å™¨"""
    
    def __init__(self, directory: str, output_file: str = None):
        self.directory = Path(directory)
        self.output_file = output_file
        self.file_hashes = defaultdict(list)
        self.scan_stats = {
            'total_files': 0,
            'total_size': 0,
            'duplicate_groups': 0,
            'duplicate_files': 0,
            'space_wasted': 0,
            'scan_time': 0
        }
    
    def calculate_file_hash(self, file_path: Path, algorithm: str = 'md5') -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_obj = hashlib.md5() if algorithm == 'md5' else hashlib.sha256()
        
        try:
            with open(file_path, 'rb') as f:
                # åˆ†å—è¯»å–ï¼Œé€‚åˆå¤§æ–‡ä»¶
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_obj.update(chunk)
            return hash_obj.hexdigest()
        except (IOError, OSError) as e:
            print(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
            return None
    
    def scan_directory(self) -> None:
        """æ‰«æç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
        print(f"å¼€å§‹æ‰«æç›®å½•: {self.directory}")
        start_time = time.time()
        
        for file_path in self.directory.rglob('*'):
            if file_path.is_file():
                self.scan_stats['total_files'] += 1
                
                try:
                    file_size = file_path.stat().st_size
                    self.scan_stats['total_size'] += file_size
                    
                    # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
                    file_hash = self.calculate_file_hash(file_path, 'md5')
                    if file_hash:
                        file_info = {
                            'path': str(file_path),
                            'name': file_path.name,
                            'size': file_size,
                            'size_mb': round(file_size / (1024 * 1024), 2),
                            'modified_time': file_path.stat().st_mtime,
                            'extension': file_path.suffix.lower()
                        }
                        self.file_hashes[file_hash].append(file_info)
                    
                    # è¿›åº¦æ˜¾ç¤º
                    if self.scan_stats['total_files'] % 100 == 0:
                        print(f"å·²æ‰«æ {self.scan_stats['total_files']} ä¸ªæ–‡ä»¶...")
                        
                except (OSError, IOError) as e:
                    print(f"è·³è¿‡æ–‡ä»¶ {file_path}: {e}")
                    continue
        
        self.scan_stats['scan_time'] = round(time.time() - start_time, 2)
        print(f"æ‰«æå®Œæˆï¼å…±å¤„ç† {self.scan_stats['total_files']} ä¸ªæ–‡ä»¶")
    
    def find_duplicates(self) -> Dict:
        """æŸ¥æ‰¾é‡å¤æ–‡ä»¶å¹¶ç”ŸæˆæŠ¥å‘Š"""
        duplicate_groups = {}
        group_id = 1
        
        for file_hash, files in self.file_hashes.items():
            if len(files) > 1:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
                files.sort(key=lambda x: x['modified_time'], reverse=True)
                
                # è®¡ç®—æµªè´¹çš„ç©ºé—´ï¼ˆé™¤äº†ä¿ç•™æœ€æ–°çš„ä¸€ä¸ªæ–‡ä»¶ï¼‰
                file_size = files[0]['size']
                wasted_space = file_size * (len(files) - 1)
                self.scan_stats['space_wasted'] += wasted_space
                
                duplicate_groups[f"group_{group_id}"] = {
                    'hash': file_hash,
                    'file_count': len(files),
                    'file_size': file_size,
                    'file_size_mb': round(file_size / (1024 * 1024), 2),
                    'wasted_space_mb': round(wasted_space / (1024 * 1024), 2),
                    'files': files,
                    'recommended_action': self._get_recommendation(files)
                }
                
                self.scan_stats['duplicate_files'] += len(files)
                group_id += 1
        
        self.scan_stats['duplicate_groups'] = len(duplicate_groups)
        self.scan_stats['space_wasted_mb'] = round(self.scan_stats['space_wasted'] / (1024 * 1024), 2)
        
        return duplicate_groups
    
    def _get_recommendation(self, files: List[Dict]) -> str:
        """æ ¹æ®æ–‡ä»¶ä¿¡æ¯ç”Ÿæˆå¤„ç†å»ºè®®"""
        if len(files) == 2:
            return "ä¿ç•™æœ€æ–°æ–‡ä»¶ï¼Œåˆ é™¤æ—§ç‰ˆæœ¬"
        elif len(files) > 2:
            return "ä¿ç•™æœ€æ–°æ–‡ä»¶ï¼Œå°†å…¶ä»–ç‰ˆæœ¬ç§»è‡³å¤‡ä»½ç›®å½•"
        else:
            return "æ— éœ€å¤„ç†"
    
    def generate_report(self) -> Dict:
        """ç”Ÿæˆå®Œæ•´çš„æ£€æµ‹æŠ¥å‘Š"""
        print("æ­£åœ¨åˆ†æé‡å¤æ–‡ä»¶...")
        duplicate_groups = self.find_duplicates()
        
        report = {
            'scan_info': {
                'directory': str(self.directory),
                'scan_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'scan_duration_seconds': self.scan_stats['scan_time']
            },
            'statistics': self.scan_stats,
            'duplicate_groups': duplicate_groups,
            'summary': {
                'has_duplicates': len(duplicate_groups) > 0,
                'total_duplicate_groups': len(duplicate_groups),
                'total_duplicate_files': self.scan_stats['duplicate_files'],
                'potential_space_saving_mb': self.scan_stats['space_wasted_mb']
            }
        }
        
        return report
    
    def save_report(self, report: Dict) -> None:
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if self.output_file:
            try:
                with open(self.output_file, 'w', encoding='utf-8') as f:
                    json.dump(report, f, ensure_ascii=False, indent=2)
                print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.output_file}")
            except IOError as e:
                print(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
    
    def print_summary(self, report: Dict) -> None:
        """æ‰“å°æ£€æµ‹ç»“æœæ‘˜è¦"""
        stats = report['statistics']
        summary = report['summary']
        
        print("\n" + "="*60)
        print("é‡å¤æ–‡ä»¶æ£€æµ‹ç»“æœæ‘˜è¦")
        print("="*60)
        print(f"æ‰«æç›®å½•: {report['scan_info']['directory']}")
        print(f"æ‰«ææ—¶é—´: {report['scan_info']['scan_time']}")
        print(f"æ‰«æè€—æ—¶: {stats['scan_time']} ç§’")
        print(f"æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
        print(f"æ€»å¤§å°: {round(stats['total_size'] / (1024*1024), 2)} MB")
        print(f"é‡å¤æ–‡ä»¶ç»„: {summary['total_duplicate_groups']}")
        print(f"é‡å¤æ–‡ä»¶æ•°: {summary['total_duplicate_files']}")
        print(f"å¯èŠ‚çœç©ºé—´: {summary['potential_space_saving_mb']} MB")
        
        if summary['has_duplicates']:
            print(f"\nå‘ç° {summary['total_duplicate_groups']} ç»„é‡å¤æ–‡ä»¶:")
            for group_name, group_info in report['duplicate_groups'].items():
                print(f"\nğŸ“ {group_name}:")
                print(f"   æ–‡ä»¶æ•°é‡: {group_info['file_count']}")
                print(f"   æ–‡ä»¶å¤§å°: {group_info['file_size_mb']} MB")
                print(f"   æµªè´¹ç©ºé—´: {group_info['wasted_space_mb']} MB")
                print(f"   å¤„ç†å»ºè®®: {group_info['recommended_action']}")
                for i, file_info in enumerate(group_info['files'][:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                    status = "ğŸ“Œ æœ€æ–°" if i == 0 else f"ğŸ“„ å‰¯æœ¬{i}"
                    print(f"   {status}: {file_info['name']}")
                    print(f"        è·¯å¾„: {file_info['path']}")
                if len(group_info['files']) > 3:
                    print(f"   ... è¿˜æœ‰ {len(group_info['files']) - 3} ä¸ªæ–‡ä»¶")
        else:
            print("\nâœ… æœªå‘ç°é‡å¤æ–‡ä»¶ï¼")
        
        print("="*60)

def main():
    parser = argparse.ArgumentParser(description='é‡å¤æ–‡ä»¶æ£€æµ‹å·¥å…·')
    parser.add_argument('--directory', '-d', required=True,
                       help='è¦æ‰«æçš„ç›®å½•è·¯å¾„')
    parser.add_argument('--output', '-o', 
                       help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)')
    parser.add_argument('--quiet', '-q', action='store_true',
                       help='é™é»˜æ¨¡å¼ï¼Œåªè¾“å‡ºæ‘˜è¦')
    
    args = parser.parse_args()
    
    # éªŒè¯ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.directory):
        print(f"é”™è¯¯: ç›®å½• '{args.directory}' ä¸å­˜åœ¨")
        return 1
    
    # è®¾ç½®é»˜è®¤è¾“å‡ºæ–‡ä»¶å
    if not args.output:
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        args.output = f"duplicate_files_report_{timestamp}.json"
    
    try:
        # åˆ›å»ºæ£€æµ‹å™¨å¹¶è¿è¡Œ
        detector = DuplicateFileDetector(args.directory, args.output)
        detector.scan_directory()
        report = detector.generate_report()
        
        # ä¿å­˜å’Œæ˜¾ç¤ºæŠ¥å‘Š
        detector.save_report(report)
        if not args.quiet:
            detector.print_summary(report)
        
        return 0
        
    except KeyboardInterrupt:
        print("\næ£€æµ‹è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"æ£€æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return 1

if __name__ == '__main__':
    exit(main())
